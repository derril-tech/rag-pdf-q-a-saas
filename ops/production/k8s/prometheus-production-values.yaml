# Created automatically by Cursor AI (2024-12-19)

# Production Prometheus configuration with comprehensive monitoring

prometheus:
  prometheusSpec:
    retention: 30d
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: gp3
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 500Gi
    resources:
      requests:
        memory: 2Gi
        cpu: 500m
      limits:
        memory: 4Gi
        cpu: 1000m
    additionalScrapeConfigs:
      - job_name: 'rag-gateway'
        static_configs:
          - targets: ['rag-gateway:9090']
        metrics_path: /metrics
        scrape_interval: 15s
      - job_name: 'rag-workers'
        static_configs:
          - targets: ['rag-workers:9090']
        metrics_path: /metrics
        scrape_interval: 15s
      - job_name: 'postgres'
        static_configs:
          - targets: ['postgres:5432']
        metrics_path: /metrics
        scrape_interval: 30s
      - job_name: 'redis'
        static_configs:
          - targets: ['redis:6379']
        metrics_path: /metrics
        scrape_interval: 30s
      - job_name: 'nats'
        static_configs:
          - targets: ['nats:8222']
        metrics_path: /metrics
        scrape_interval: 30s

alertmanager:
  alertmanagerSpec:
    retention: 120h
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: gp3
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    resources:
      requests:
        memory: 100Mi
        cpu: 100m
      limits:
        memory: 200Mi
        cpu: 200m
    config:
      global:
        slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'
        smtp_smarthost: 'smtp.gmail.com:587'
        smtp_from: 'alerts@yourapp.com'
        smtp_auth_username: 'alerts@yourapp.com'
        smtp_auth_password: 'your_smtp_password'
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 1h
        receiver: 'slack-notifications'
        routes:
          - match:
              severity: critical
            receiver: 'pagerduty-critical'
            continue: true
          - match:
              severity: warning
            receiver: 'slack-notifications'
      receivers:
        - name: 'slack-notifications'
          slack_configs:
            - channel: '#alerts'
              title: '{{ template "slack.title" . }}'
              text: '{{ template "slack.text" . }}'
              send_resolved: true
        - name: 'pagerduty-critical'
          pagerduty_configs:
            - routing_key: 'your_pagerduty_key'
              description: '{{ template "pagerduty.description" . }}'
              severity: '{{ if eq .CommonLabels.severity "critical" }}critical{{ else }}warning{{ end }}'
        - name: 'email-notifications'
          email_configs:
            - to: 'ops@yourapp.com'
              send_resolved: true

grafana:
  adminPassword: 'your_secure_grafana_password'
  persistence:
    enabled: true
    storageClassName: gp3
    size: 10Gi
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      memory: 512Mi
      cpu: 200m
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      rag-overview:
        gnetId: 1
        revision: 1
        datasource: Prometheus
      rag-performance:
        gnetId: 2
        revision: 1
        datasource: Prometheus
      system-metrics:
        gnetId: 3
        revision: 1
        datasource: Prometheus

# Custom alerting rules
additionalPrometheusRulesMap:
  rag-alerts:
    groups:
      - name: rag.general
        rules:
          # High error rate
          - alert: HighErrorRate
            expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "High error rate detected"
              description: "Error rate is {{ $value }} errors per second"
          
          # High response time
          - alert: HighResponseTime
            expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High response time detected"
              description: "95th percentile response time is {{ $value }} seconds"
          
          # Database connection issues
          - alert: DatabaseConnectionIssues
            expr: pg_up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Database connection issues"
              description: "PostgreSQL is down"
          
          # Redis connection issues
          - alert: RedisConnectionIssues
            expr: redis_up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Redis connection issues"
              description: "Redis is down"
          
          # Worker queue backlog
          - alert: WorkerQueueBacklog
            expr: nats_queue_messages > 1000
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Worker queue backlog detected"
              description: "Queue has {{ $value }} messages"
          
          # High memory usage
          - alert: HighMemoryUsage
            expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.85
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High memory usage detected"
              description: "Container memory usage is {{ $value | humanizePercentage }}"
          
          # High CPU usage
          - alert: HighCPUUsage
            expr: (rate(container_cpu_usage_seconds_total[5m]) / container_spec_cpu_quota) > 0.8
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage detected"
              description: "Container CPU usage is {{ $value | humanizePercentage }}"
          
          # Disk space running low
          - alert: DiskSpaceLow
            expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Disk space running low"
              description: "Disk usage is {{ $value | humanizePercentage }}"
          
          # Pod restarting frequently
          - alert: PodRestartingFrequently
            expr: increase(kube_pod_container_status_restarts_total[15m]) > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Pod restarting frequently"
              description: "Pod has restarted {{ $value }} times in the last 15 minutes"
          
          # Service unavailable
          - alert: ServiceUnavailable
            expr: up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Service unavailable"
              description: "Service {{ $labels.job }} is down"
